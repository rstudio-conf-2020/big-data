```{r, spark-caching, include = FALSE}
eval_caching <- FALSE
if(Sys.getenv("GLOBAL_EVAL") != "") eval_caching <- Sys.getenv("GLOBAL_EVAL")
```

# Spark data caching

## Map data
*See the machanics of how Spark is able to use files as a data source*

1. Examine the contents of the **data** folder

1. Load the `sparklyr` library
    ```{r, eval = eval_caching}
    library(sparklyr)
    ```

1. Use `spark_connect()` to create a new local Spark session
    ```{r, eval = eval_caching}
    sc <- spark_connect(master = "local")
    ```

1. Load the `readr` and `purrr` libraries
    ```{r, eval = eval_caching}
    library(readr)
    library(purrr)
    ```
    
1. Read the top 5 rows of the **transactions_1** CSV file
    ```{r, eval = eval_caching}
    top_rows <- read_csv("data/transactions_1.csv", n_max = 5)
    ```

1. Create a list based on the column names, and add a list item with "character" as its value. Name the variable `file_columns`
    ```{r, eval = eval_caching}
    file_columns <- top_rows %>%
      rename_all(tolower) %>%
      map(function(x) "character")
    ```
    
1. Preview the contents of the `file_columns` variable
    ```{r, eval = eval_caching}
    head(file_columns)
    ```

1. Use `spark_read()` to "map" the file's structure and location to the Spark context. Assign it to the `spark_transactions` variable
    ```{r, eval = eval_caching}
    spark_transactions <- spark_read_csv(
      sc,
      name = "transactions",
      path = "data",
      memory = FALSE,
      columns = file_columns,
      infer_schema = FALSE
    )
    ```

1. In the Connections pane, click on the table icon by the `transactions` variable

1. Verify that the new variable pointer works by using `tally()`
    ```{r, eval = eval_caching}
    spark_transactions %>%
      tally()
    ```

## Caching data
*Learn how to cache a subset of the data in Spark*

1. Create a subset of the *flights* table object
    ```{r, eval = eval_caching}
    spark_orders <- spark_transactions %>%
      mutate(price = as.double(price)) %>%
      group_by(order_id) %>%
      summarise(total_sales = sum(price, na.rm = TRUE), no_items = n())
    ```

1. Use `compute()` to extract the data into Spark memory
    ```{r, eval = eval_caching}
    cached_orders <- compute(spark_orders, "orders")
    ```

1. Confirm new variable pointer works
    ```{r, eval = eval_caching}
    head(spark_orders)
    ```

1. Go to the Spark UI 

1. Click the **Storage** button

1. Notice that "orders" is now cached into Spark memory 

